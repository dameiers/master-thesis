% mention the early beginning static documents, klick and load cylce
% mention the different techniques for dynamic generated html documents (client side, server side) (see vergleich_web_tech.)

\chapter{Brief examination on Web Technologies}\label{chap:web_dev}

In this chapter a brief overview of the state of the art in web development is given. The different technologies are discussed 
in regard to their abilities of building web application with rich interaction possibilities similar to native desktop applications and the integration in the navigator GUI in particular. This shall reveal the most appropriate technology for building web applications that can be integrated in a seamless manner. 

The technologies (programming languages, standards, protocols etc) that can be used for building web applications is manifold. For example there are PHP, Python, Perl, Ruby,Java, .Net, HTML,CSS and JavaScript. To build a web application normally a combination of multiple of these technologies are used. This makes it very difficult to select the proper combination of technologies. The situation gets even worse, if the existing web application frameworks, that are normally used when building more complex and professional web applications, are taken into consideration. A good overview over existing frameworks can be found at [Wikipedia List of Web Application Frameworks]

Although this diversity of different technologies, the initial architecture of the world wide web is still unchanged. It allows a characterisation and classification of the different technologies. The underlying architecture of the world wide web is is a client-server architecture which allows us to separate the various technologies in client and server side technologies. Picture XY depicts this separation and lists a (subset) of web technologies. 

\begin{itemize}
	\item{server side:
		\begin{itemize}
			\item{php}
			\item{python}
			\item{perl}
			\item{Ruby}
			\item{ASP.NET}
			\item{php}
			\item{java Server Faces}
		\end{itemize} 
	}
\item{client side:
		\begin{itemize}
			\item{HTML}
			\item{CSS}
			\item{JavaScript}
			\item{AJAX}
			\item{DOM}
			\item{Java Applets}
			\item{Flash / Air / Flex}
			\item{Silverlight}
			\item{JavaFX}
		\end{itemize} 
	}
\end{itemize}

The client server architecture of the web also dictates the basic communication process. The initial idea of the world wide web is that the client requests a static web page from a web server. This static document contains styling information in form of HTML that can be interpreted by the client, the web browser, and is used to visualise the document. For nearly every user interaction the client sends new request and the process repeats. First with the introduction of Ajax by Jessie James Garret \autocite[]{ajax} it was initially possible to load data asynchronous and programatically from the server and to refresh the current page partially. Ajax is a client side technology hence it allows the client to send a request and process its result asynchronously. Usually Ajax works tightly together with JavaScript.
%reference to page load cycle.


% disadvantage of server side technologies: 
% 	total page refresh for every request
% 	additional server that holds the logic is needed, makes integration more complex
%  	in our case we already have a backend (RESTful API, cids server) 
A pure server-side application uses one of the programming languages like PHP, Perl or Ruby to dynamically generate a HTML document that is send to the client. The client still just has to interpret and visualise HTML documents. The total application/business logic is handled by the server and every click on the page forces a reload of the total page. All these points are disadvantageous when trying to integrate server driven web applications into a native Swing GUI.
Refreshing the whole application after every user interaction is in high contrast with the partial refresh of native desktop applications and needs to be avoided for a uniform user experience. Current web frameworks admittedly offer ways to avoid this but only by using client side scripting and Ajax. Another more important issue is, that always an additional web server is required, which is responsible for data management, application logic and session management which makes the interaction with the navigator GUI more difficult. For example how can the navigator be informed about data changes made in the web application? Furthermore, the most server side frameworks are intended to built full stack applications which means that they are also facilitate the creation of data models, persistence, authentication and son on. This doesn't really match the situation, since those functionality is already implemented in the cids server respectively the new cids RESTful API which always provide a fix interface for the client.
%ruby on rails as an example? its a good framework for building good / modern / perfomant web applications, but its more suited when it is necessary to devlop business logic and 

As already mentioned, rich user experience and interaction possibilities can only be implemented with client side technologies. For a long time there were significant differences in the user interaction of web applications and native desktop applications like Swing. This is the reason why technologies like Adobe Flash and Java Applets came up and coined a totally new kind of web applications that should leave the drawbacks behind.

%With pure server side applications it is difficult or impossible to built web applications that have an approximate level of user interaction as native desktop applications owing to the fact that they deliver dynamically generated but still static HTML documents. Developing dynamic web applications with rich user interaction always requires client side technology such as CSS and JavaScript. Quite natural, nearly all server side web frameworks support building dynamic web pages today by integrating support for JavaScript. Ruby on Rails is a good example for that. Although Ruby on Rails is a server side technology it brings support for prototype.js which is a JavaScript framework and makes it possible to write dynamic and user friendly web applications.


% For a long time there were significant differences in web based and native deskopt applications (Why-> missing interaction concepts like no D&D, No Asynchronous communication with the server, not possible to load onyl data...)
%Fortunately those drawb

\section{Rich Internet Applications}

\subsection{Definition and Characteristics}

The term Rich Internet Application (RIA) was firstly introduced by Jeremy Allaire. In \autocite[]{allaire_ria} he describes important characteristics of rich internet applications such as a powerful and extensible model for interactivity, using web and data services provided by application servers etc. The term RIA is not standardized and there exist various definitions often with only a slight difference. 

Bozzon et al state that RIAs are  \enquote{[...] a variant of Web-based systems providing sophisticated interfaces for representing complex processes and data, minimizing client-server data transfers and moving the interaction and presentation layers from the server to the client.} \autocite[]{ria-definition-1}
 
A good discussion on the various definitions can be found at \autocite[]{ria-state-of-the-art}. They conclude that there are two parts that distinguishes RIA from "normal" web applications, a technical part and the user experience. The technical differences are first and foremost the asynchronous data exchange with the server and the shift of application logic to the client. The second part comprises the fact, that RIA's look and behave more like native desktop applications, in fact they have a richer user interaction and can be used on and offline.

\todo{say that this is what wee need}

 
\subsection{Technologies}

There are two groups of technologies with that rich internet applications can be build and that have evolved with the progress of client side web development technologies. Bozzon et al \autocite{ria-classification-1} defines 4 categories of RIA technologies,scripting-based, plugin-based (Flash, Silverlight), browser-based (XUL, XAML) and web-based desktop technologies (Java Web Start). A similar categorization can also be found in \autocite{ria-classification-2}. The definition of web-based desktop technologies like Java WebStart as a RIA technology should be called into question. Such applications are normal desktop applications, in case of Java Web Start Swing applications that are not executed in the browser an require a special runtime environment. They merely use the web as deployment mechanism but can not be regarded as web applications. Browser based RIA are more seldom which may be caused in the fact that they only run in a specific browser. Both are not suited for building web applications that can be used in the navigator Swing based GUI and as normal web application. The most important types are plugin-based and sripting-based RIA's. 

Plugin-based RIA's, as the name suggest, need a special and often vendor specific browser plugin or runtime environment. Prominent candidates are Adobe Flash, Adobe Air, Microsoft Silverlight and JavaFX. In \autocite[]{ria-comp-1} and \autocite[]{ria-comp-2} a good comparison of the most prominent plugin based RIA technologies can be found.

There are many issues with plugin based RIA's platforms. The most important one is, that always an additional Browser plugin is needed. This limits the accessibility of web application and is contradictionary with the easy and wide spread acessibility of web applications. Depending on the Platform it can happen that there is no plugin for every browser or operating system. Even if there is a corresponding plugin available, it can not be guaranteed that the plugin is installed. A good example for this are mobile devices. There is no support for Flash on any iOS device. In \autocite[]{jobs-thoughts-on-flash} Steve Jobs explain the reasons for this. Some of the reason he mention can also be transferred to other plugin based RIA platforms. For example Jobs criticized that Flash \enquote{[...] has not performed well on mobile devices} \autocite[]{jobs-thoughts-on-flash}. This issue belongs to all plugin based RIA platforms and and is not strictly limited to mobile devices. They need a long time to load and initialize the application. In addition, there is a considerably increased security risk with one or more browser plugins installed. Flash in particular is well known for its security vulnerabilities. And finally most of the above mentioned platforms are not open source or standardized.

Regarding all these issues and the latest improvements in web standards such as HTML5, CSS3 and JavaScript it is not surprising that the stake of web applications that use a plugin based RIA platform has decreased rapidly in the last years. Figure \ref{flash_usage} depicts the current usage of Flash and Silverlight and gives a back sight for the last three years. From special interest is the decreasing stake of Adobe Flash.

\begin{figure}
	\centering \includegraphics[width=0.8\textwidth]{./img/flash_usage.png}
	\caption{usage client side web technologies \autocite[]{ria_flash-usage}}
	\label{fig:flash_usage}
\end{figure}

%What are the advantages of 
The last group of RIA's are solely implemented with web standards such as HTML5, CSS and JavaScript and make highly use of AJAX. AJAX and JavaScript are the most important technologies for building web applications with a rich user interface in fact AJAX changes the way the client communicates with the server as already mentioned. The distinction The crucial advantage is that no special plugin or runtime environment is required, and scripting-based RIA's work in all standard conform web browser and on all devices. This ensures that they have the same accessibility like normal web applications. 

\todo{better emphasize the advantages of JavaScript RIA}
The immense improvements in the last years regarding new standards like HTML 5, CSS3 and JavaScript remedy a lot of the till then valid drawbacks of scripting based RIAs'. Just to give an example: Although they totally rely on standardized technologies, there are many differences how these standards are implemented in the different browsers, why it was often necessary to write browser depended code. The principal issue still exists today,but owing to the use of frameworks that abstract from the underlying browser those problems can be neglected. Additionally, the rapid performance improvements of JavaScript engines in the last years, remedy the performance problem of pure script-based RIA's. Another problem of script-based applications was, that they didn't have the same level of interaction richness. But also this changed with the progresses that were achieved with JavaScript. If we regards web applications like Google Mail, Google Drive, Facebook and so on, it is clear that it is possible to write rich web applications solely with web standards and that behave and act like native desktop applications. With upcoming technologies like local storage, client side data bases will even allow to develop web applications that can be used offline. Web Sockets will allow a bi-directional communication between the server and the client and with web workers will extend JavaScript with an asynchronous execution model.

As already mentioned there are many efforts in building JavaScript frameworks and libraries that shall facilitate the development of rich JavaScript and HTML5 based web applications. In the last year a new trend in that sector are frameworks that use the MVC pattern to structure the code and keep it maintainable and increase productivity. This stands in contrast to the more matured Frameworks like JQuery, YUI and Prototype which rather focuses on hiding browser specific implementation details and the DOM. A detailed comparison of the actual existing JavaScript MVC is given in chapter \ref{chap:detail_comparison}. The next chapter however, shall give a more conceptual overview over these new kind of frameworks.
 
\section{JavaScript MVC-Frameworks}

As already mentioned modern JavaScript Frameworks like AngularJS, Ember and Knockout try to structure the code by using architectural patterns like MVC. In order of doing this, they achieve a separation of presentation logic, business logic and presentation state. Furthermore it help developers to understand the framework much faster and get more comfortable with the framework hence the pattern gives them already a well known mental model. Unfortunately not all frameworks use the classical MVC pattern as it is defined in \autocite[]{smalltalk_mvc}. Some of them use slightly variants of this pattern like MVVM (Model-View-ViewModel or MVP (Model View Presenter). Knockout for example uses the MVVM pattern to enable a loose coupling of the domain model and the view (cf. \autocite{heise_knockout}).  

The usage of the different pattern variations often lead to discussions of the various advantages and disadvantages of the used pattern and shifted the focus away from the frameworks itself. But in fact all of the used patterns basically entail the same advantages, an Angular developer introduced the term MVW. In he argues \enquote{Having said, I'd rather see developers build kick-ass apps that are well-designed and follow separation of concerns, than see them waste time arguing about MV* nonsense. And for this reason, I hereby declare AngularJS to be MVW framework - Model-View-Whatever. Where Whatever stands for "whatever works for you".} \autocite[]{anguler_mvw}. This opinion finds more and more acceptance.

%Data binding => different approaches: using special Objects (Ember, Knockout,) vs Dirty Checking (Angular)
Another important feature that can be seen as an key point of the new frameworks is automatic (two way) data binding between the model and the view. This means that it is possible to bind UI elements like texts, or input fields to properties of the correlating model. Changes in the UI, for example user input, is automatically reflected to the model property, and programmatically changing the property updates the user interface. \todo{example with images} This is an immense advancement in contrast to more ancient frameworks in fact the developer isn't responsible to write and test code for that purpose by hand or need to ensure manually that changes are reflected. While there are multiple ways to achieve this, there are mainly two different approaches how the two way data binding is implemented. A good and detailed examination of those implementations can be found in \autocite[]{binding_comparison}. One approach is to use special java script objects on the model site that are able to notify observers (the view) about changes. This approach is used for example by EmberJS and Knockout. The second approach allows to bind directly to plain old JavaScript objects. Hence at the current JavaScript version it is not possible to get notified about changes of an objects value, a different approach is needed to detect changes. The technique used there is periodically checking if any of the bound JavaScript objects has changed and updating the view for every changed object. This process is often called Dirty Checking.
The two approaches have some fundamental consequences that are discussed into more detail in chapter \ref{chap:detail_comparison}
\todo{mention new feature Object.observe() }

\todo{put this into chapter 4}
The two approaches have some fundamental consequences. The first and most obvious one is that using dirty checking means that you can bind directly to native JavaScript objects. This makes the binding expressions slightly more intuitive since no get or set functions are needed to access the properties. Furthermore it is not necessary to wrap any JavaScript object into an framework depended observable properties. On the other side the general approach to periodically check the bound JavaScript objects for changes seems to be time consuming and slow. Although this is conceptually true, this statement needs to be qualified in the practical context. A good discussion regarding this issue can be found at \todo{link to stackoverflow issue}. The main arguments mentioned there are, that dirty checking is only problematic for a large number of objects and that the human eye can not recognize changes faster than a 50 ms. There need to be a large amount of objects to check before the 50 ms are exceeded and the user can recognize a sluggish UI. Another important difference is how computed properties are implemented. A computed property is a property that depends on other properties. Figure \todo{add figure and reef} demonstrates this. Hence dir

Very closely related to the two way data binding feature is the usage of template engines in modern JavScript MVC frameworks. Templates support the developer in separating style and design from application/business logic. Furthermore they make it unnecessary to programmatically generate html markup. The template consists of html markup and some further place holders or variables where the final data gets inserted. 
The engines differ in the possibilities to integrate logic inside the templates. some allow full and arbitrary JavaScript logic, other  a small set of constructs offered by the template engine itself, like loops, conditionals or partials. The latter approach is the one that is mostly used, hence using arbitrary JavaScript in the templates is contrast with the initial idea of separating html markup and code. 
Another distinction is if the template engine is string based or DOM based. \todo{explain}
\todo{mention new HTML tag template}

The shift of the whole application logic to the client and only fetching data from the client creates also some new problems. 
Hence the total application is loaded with the first request, no entries are added to the browsers history during the usage of the web application. This destroys the browsers history and with it important functions like using the back and forward button or bookmarking certain states of the application. This problem leads us the next core component that most of the modern frameworks provide, the routing mechanism. Routing means that the application maps the different states of the application into an URL that can be used for the browsers history. The first approach that is used today is using the \'\#\' fragment identifier of the URL \todo{add ref to single page manifesto}. The fragment identifier usually refers to an HTML element id or a named anchor in the current page and allows the automatic scrolling to the location of that HTML element. Current JavaScript frameworks use the fragment identifier to track the state of the application and the routers are responsible for turning the fragment identifier part of the URL to the right application state. Using the fragment identifier for tracking application state is highly controversial. There are a lot of blog posts and comments that discusses this approach (cf. \autocite[]{hashbang_urls_1},\autocite[]{hashbang_urls_2},\autocite[]{hashbang_urls_3}). To put the discussion in a nutshell the main arguments against this approach are that it changes the target of the url, hence the fragment identifier doesnâ€™t get send to the server, and only clients with JavaScript enabled can interpret these kind of URLs. Using hash-bang URLs was the only possibility for tracking state in client side JavaScript applications. Fortunately the upcoming HTML 5 standard contains a new part, the History API \autocite[]{w3c-history-api}, that allows the browser to modify the current URL and the browser history programmatically. The HTML 5 History API offers a new way for tracking application state without using hash-bang URLs and the related drawbacks. Hence most browsers already support the History API (see also figure \ref{fig:usage_history_api}) it is just a matter of time till this feature replace hash-bang URLs in the frameworks. Angular for example already offers the possibility to already use the History API (cf. \autocite[]{angular_location}). 

\begin{figure}
	\centering \includegraphics[width=0.8\textwidth]{./img/web-dev/usage_history_api.png}
	\caption{Browser Support History API \autocite{can_i_use}}
	\label{fig:usage_history_api}
\end{figure}

Another problem that comes up using JavaScript driven web applications is that they can't be analysed from search engines as easily as server centric web applications hence the initial page request doesn't return a HTML document that contains the important content directly. But also for this drawback exists certain technologies that remedy this disadvantage. The first on was announced from Google in 2009. In \autocite[]{google_AJAX_crawling} they announce a method to crawl hash-bang URLs. The main idea is that the crawler maps the hash-bang URL into an normal URL that contains the fragment identifier. Figure \ref{fig:google_crawling_ajax} depicts the idea. Important to note is that the web server needs to deliver a custom and search engine optimized HTML document for the URL. This is what all approaches have in common. For example an other upcoming approach that gets more and more popular is to use a headless web browser like PhantomJS that is able to parse the web application like a normal browser and can output the normal HTML markup to the search bot.

\begin{figure}
	\centering \includegraphics[width=0.8\textwidth]{./img/web-dev/google_crawling_hashbang.png}
	\caption{Google crawling AJAX \autocite{google_AJAX_crawling}}
	\label{fig:google_crawling_ajax}
\end{figure}

    
